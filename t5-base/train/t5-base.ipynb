{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8bf427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/anaconda3/envs/t5/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/med/anaconda3/envs/t5/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/med/anaconda3/envs/t5/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "from torch import nn\n",
    "from transformers import T5Model\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# from sacrebleu.metrics import BLEU\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import jieba\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d08010",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dataset_size = 200000\n",
    "max_input_length = 512\n",
    "max_target_length = 32\n",
    "stride = 128\n",
    "train_batch_size = 8\n",
    "valid_batch_size = 8\n",
    "learning_rate = 1e-5\n",
    "epoch_num = 10\n",
    "beam_size = 4\n",
    "no_repeat_ngram_size = 2\n",
    "\n",
    "seed = 5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "save_dir = \"/home/med/selflearning/stage1-t5/model/bin\"\n",
    "output_dir = '/home/med/selflearning/stage1-t5/data/DuReaderQG/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb71efb3",
   "metadata": {},
   "source": [
    "# 1. 准备数据\n",
    "## 1.1. 构建数据集\n",
    "先编写继承自 Dataset 类的自定义数据集类用于组织样本和标签\n",
    "\n",
    "### 合并验证集\n",
    "训练集只有一个答案，验证集一个问题可能对应有多个参考答案，将相同question和context的所有answer合并进一个answers列表。\n",
    "\n",
    "验证集数据格式：\n",
    "\n",
    "`{\"context\": \"还没有最后确定，暂定2017年\", \"question\": \"余罪第三季开播时间\", \"answers\": [\"暂定2017年\", \"2017年\"], \"id\": 9}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f907ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5(Dataset):\n",
    "    def __init__(self, data_file, merge=False):\n",
    "        \"\"\"\n",
    "        data_file: 输入文件\n",
    "        merge: 是否合并相同 (context, question) 的答案\n",
    "        \"\"\"\n",
    "        self.data = self.load_data(data_file, merge)\n",
    "\n",
    "    def load_data(self, data_file, merge):\n",
    "        if not merge:\n",
    "            # 不合并，直接一行一个样本\n",
    "            Data = []\n",
    "            with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "                for idx, line in enumerate(f, start=1):\n",
    "                    sample = json.loads(line.strip())\n",
    "                    sample[\"id\"] = idx\n",
    "                    Data.append(sample)\n",
    "            return Data\n",
    "        else:\n",
    "            # 合并，相同 (context, question) 聚合答案\n",
    "            merged = defaultdict(list)\n",
    "            with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    sample = json.loads(line.strip())\n",
    "                    key = (sample[\"context\"], sample[\"question\"])\n",
    "                    merged[key].append(sample[\"answer\"])\n",
    "            Data = []\n",
    "            for new_id, ((context, question), answers) in enumerate(merged.items(), start=1):\n",
    "                Data.append({\n",
    "                    \"context\": context,\n",
    "                    \"question\": question,\n",
    "                    \"answers\": answers,  # list of answers\n",
    "                    \"id\": new_id\n",
    "                })\n",
    "            return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90df3c7",
   "metadata": {},
   "source": [
    "### 分割测试集\n",
    "train 的 10% 作为测试集\n",
    "- Train data size: 13068\n",
    "- Valid data size: 700\n",
    "- Test data size: 1452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcde9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = T5('/home/med/selflearning/stage1-t5/data/DuReaderQG/train.json', merge=False)\n",
    "valid_data = T5('/home/med/selflearning/stage1-t5/data/DuReaderQG/dev.json', merge=True)\n",
    "\n",
    "indices = list(range(len(train_dataset)))\n",
    "n_test = int(len(indices) * 0.1)\n",
    "test_data  = Subset(train_dataset, indices[:n_test])\n",
    "train_data = Subset(train_dataset, indices[n_test:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6043d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 13068\n",
      "Valid data size: 700\n",
      "Test data size: 1452\n",
      "{'context': '晚上十点。以下是身体器官工作时间表： 一、晚上9-11点为免疫系统（淋巴）排毒时间，此段时间应安静或听音乐。二、晚间11-凌晨1点，肝的排毒，需在熟睡中进行。三、凌晨1-3点，胆的排毒，亦同。四、凌晨3-5点，肺的排毒。此即为何咳嗽的人在这段时间咳得最剧烈，因排毒动作已走到肺。五、凌晨5-7点，大肠的排毒，应上厕所排便。六、凌晨7-9点，小肠大量吸收营养的时段，应吃早餐。疗病者最好早吃，在6点半前，养生者在7点半前，不吃早餐者应改变习惯，即使拖到9、10点吃都比不吃好。七、半夜至凌晨4点为脊椎造血时段，必须熟睡，不宜熬夜', 'answer': '晚上十点', 'question': '几点开始算熬夜', 'id': 1453}\n",
      "{'context': '年基准利率4.35%。 从实际看,贷款的基本条件是: 一是中国大陆居民,年龄在60岁以下; 二是有稳定的住址和工作或经营地点; 三是有稳定的收入来源; 四是无不良信用记录,贷款用途不能作为炒股,赌博等行为; 五是具有完全民事行为能力。', 'question': '2017年银行贷款基准利率', 'answers': ['年基准利率4.35%', '4.35%'], 'id': 1}\n",
      "{'context': '第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。', 'answer': '第35集', 'question': '仙剑奇侠传3第几集上天界', 'id': 1}\n"
     ]
    }
   ],
   "source": [
    "# Check data size and sample\n",
    "print(f'Train data size: {len(train_data)}')\n",
    "print(f'Valid data size: {len(valid_data)}')\n",
    "print(f'Test data size: {len(test_data)}')\n",
    "\n",
    "print(next(iter(train_data)))\n",
    "print(next(iter(valid_data)))\n",
    "print(next(iter(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc4b22",
   "metadata": {},
   "source": [
    "## 1.2. 数据预处理\n",
    "### T5Tokenizer\n",
    "AutoTokenizer -> T5Tokenizer\n",
    "> 第五章：模型与分词器\n",
    "> \n",
    "> 调用 Tokenizer.save_pretrained() 函数会在保存路径下创建三个文件：\n",
    "> \n",
    "> - special_tokens_map.json：映射文件，里面包含 unknown token 等特殊字符的映射关系；\n",
    "> - tokenizer_config.json：分词器配置文件，存储构建分词器需要的参数；\n",
    "> - vocab.txt：词表，一行一个 token，行号就是对应的 token ID（从 0 开始）。\n",
    ">\n",
    "- ❌ mengzi使用modelscope pipeline，没有tokenizer.json，Autotokenizer加载失败\n",
    "- ✅ 明确使用 T5Tokenizer，避免 AutoTokenizer 转换失败"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c41e661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"/home/med/selflearning/stage1-t5/model/mengzi-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e311626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overflowing_tokens': [], 'num_truncated_tokens': -110, 'input_ids': [9595, 172, 1044, 7419, 1, 1263, 750, 101, 4, 10687, 802, 7146, 19063, 715, 13, 7, 39, 6, 1263, 280, 18425, 101, 18, 19444, 22, 17152, 25, 11445, 133, 3, 439, 976, 133, 268, 5544, 74, 24432, 4, 195, 6, 16956, 506, 114, 7650, 94, 101, 3, 2425, 5, 11445, 3, 1617, 8, 2003, 1533, 16, 103, 4, 105, 6, 7650, 18214, 101, 3, 4208, 5, 11445, 3, 1295, 365, 4, 244, 6, 7650, 14539, 101, 3, 2967, 5, 11445, 4, 439, 397, 3089, 7466, 123, 8, 8484, 9898, 116, 126, 10700, 3, 293, 11445, 1824, 231, 4338, 2967, 4, 270, 6, 7650, 108, 13357, 101, 3, 15454, 5, 11445, 3, 268, 16511, 18685, 4, 544, 6, 7650, 219, 14660, 101, 3, 23982, 1444, 2534, 1986, 5, 10416, 3, 268, 25920, 4, 9670, 389, 117, 1191, 927, 208, 3, 8, 173, 10160, 111, 3, 3772, 117, 8, 219, 10160, 111, 3, 20, 25920, 117, 268, 829, 1403, 3, 1335, 3044, 38, 280, 6, 165, 101, 208, 47, 151, 7258, 67, 4, 744, 6, 8572, 229, 7650, 138, 101, 18, 22461, 22901, 10416, 3, 557, 2003, 1533, 3, 5258, 7419, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['几点', '开始', '算', '熬夜', '</s>', '晚上', '十', '点', '。', '以下是', '身体', '器官', '工作时间', '表', ':', '▁', '一', '、', '晚上', '9', '-11', '点', '为', '免疫系统', '(', '淋巴', ')', '排毒', '时间', ',', '此', '段', '时间', '应', '安静', '或', '听音乐', '。', '二', '、', '晚间', '11', '-', '凌晨', '1', '点', ',', '肝', '的', '排毒', ',', '需', '在', '熟', '睡', '中', '进行', '。', '三', '、', '凌晨', '1-3', '点', ',', '胆', '的', '排毒', ',', '亦', '同', '。', '四', '、', '凌晨', '3-5', '点', ',', '肺', '的', '排毒', '。', '此', '即', '为何', '咳嗽', '的人', '在', '这段时间', '咳', '得', '最', '剧烈', ',', '因', '排毒', '动作', '已', '走到', '肺', '。', '五', '、', '凌晨', '5', '-7', '点', ',', '大肠', '的', '排毒', ',', '应', '上厕所', '排便', '。', '六', '、', '凌晨', '7', '-9', '点', ',', '小肠', '大量', '吸收', '营养', '的', '时段', ',', '应', '吃早餐', '。', '疗', '病', '者', '最好', '早', '吃', ',', '在', '6', '点半', '前', ',', '养生', '者', '在', '7', '点半', '前', ',', '不', '吃早餐', '者', '应', '改变', '习惯', ',', '即使', '拖', '到', '9', '、', '10', '点', '吃', '都', '比', '不吃', '好', '。', '七', '、', '半夜', '至', '凌晨', '4', '点', '为', '脊椎', '造血', '时段', ',', '必须', '熟', '睡', ',', '不宜', '熬夜', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Check the tokenization result\n",
    "sample = train_data[0]\n",
    "context = sample['context']\n",
    "question = sample['question']\n",
    "answer = sample['answer']\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=300,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=50,\n",
    ")\n",
    "\n",
    "print(inputs)\n",
    "print(tokenizer.convert_ids_to_tokens(inputs.input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb43787",
   "metadata": {},
   "source": [
    "### T5训练模板\n",
    "> 第四章：开箱即用的 pipelines / 这些 pipeline 背后做了什么？\n",
    "> 1. 预处理 (preprocessing)，将原始文本转换为模型可以接受的输入格式；\n",
    "> 2. 将处理好的输入送入模型；\n",
    "> 3. 对模型的输出进行后处理 (postprocessing)，将其转换为人类方便阅读的格式\n",
    ">\n",
    "\n",
    "- inputs: `question <extra_id_0> context`\n",
    "- targets: `<extra_id_0> answer <extra_id_1>`\n",
    "\n",
    "### collate_fn\n",
    "> 直接使用 Transformers 库自带的 AutoModelForSeq2SeqLM 函数来构建模型，因此我们需要将每一个 batch 中的数据处理为该模型可接受的格式：一个包含 'input_ids'、'attention_mask'、'labels' 和 'decoder_input_ids' 键的字典。\n",
    "\n",
    "> 与我们之前任务中使用的纯 Encoder 模型不同，Seq2Seq 任务对应的模型采用的是 Encoder-Decoder 框架：Encoder 负责编码输入序列，Decoder 负责循环地逐个生成输出 token。因此，对于每一个样本，我们还需要额外准备 decoder input IDs 作为 Decoder 的输入。decoder input IDs 是标签序列的移位，在序列的开始位置增加了一个特殊的“序列起始符”。\n",
    ">\n",
    "\n",
    "这里用了 `text_target=`，它会在返回的 `batch_data` 里 自动生成：\n",
    "- `input_ids`（来自 `batch_inputs`）\n",
    "- `labels`（来自 `text_target`，已经右移处理）\n",
    "训练时，`T5ForConditionalGeneration` 的 `forward()` 会根据 `labels` 自动构造 `decoder_input_ids`（内部调用 `shift_tokens_right`）。不用手动传 `decoder_input_ids`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63891c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['question'].strip() + ' <extra_id_0> ' + sample['context'].strip())\n",
    "        batch_targets.append('<extra_id_0> ' + sample['answer'].strip() + ' <extra_id_1>')\n",
    "\n",
    "    # print(batch_inputs)\n",
    "    # print(batch_targets)\n",
    "    \n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs,               # 作为 encoder 的输入文本列表\n",
    "        text_target=batch_targets,  # 作为 decoder 的目标文本（transformers 的 T5 支持 text_target 直接生成 labels，已经右移处理）\n",
    "        padding=True,               # 对 batch 内样本进行 padding 到相同长度（默认 pad 到最长样本长度）\n",
    "        max_length=max_input_length,    # 超过该长度则截断\n",
    "        truncation=True,            # 允许截断\n",
    "        return_tensors=\"pt\"         # 返回 PyTorch 张量（dict 中包含 input_ids, attention_mask, labels 等）\n",
    "    )\n",
    "\n",
    "    batch_data['labels'][batch_data['labels'] == tokenizer.pad_token_id] = -100 # 将 labels 中等于 pad_token_id 的位置替换为 -100\n",
    "    \n",
    "    return batch_data               # 返回一个 dict（PyTorch 张量），可以直接被模型的 forward(**batch_data) 或 Trainer 使用。\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,                     # 训练数据集（实现了 __len__ 和 __getitem__）\n",
    "    batch_size=train_batch_size,    # 每个 batch 的样本数\n",
    "    shuffle=True,                   # 每个 epoch 前打乱数据（训练时常用）\n",
    "    collate_fn=collate_fn           # 自定义的 collate 函数（用于将多个样本合并为一个 batch）\n",
    "    )\n",
    "    \n",
    "test_dataloader = DataLoader(\n",
    "    test_data,                     # 测试数据集\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39ec22d",
   "metadata": {},
   "source": [
    "### collate_fn_valid\n",
    "- tensor_data：tensor 类型的数据（模型输入、labels）\n",
    "\n",
    "- list_data：list 类型的数据（多参考答案 all_answers）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268692c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_valid(batch_samples):\n",
    "    \"\"\"\n",
    "    专门用于验证集的 collate_fn，处理多参考答案\n",
    "    \"\"\"\n",
    "    batch_inputs, batch_targets, batch_all_answers = [], [], []\n",
    "\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['question'].strip() + ' <extra_id_0> ' + sample['context'].strip())\n",
    "        batch_targets.append('<extra_id_0> ' + sample['answers'][0].strip() + ' <extra_id_1>')  # 用第一个答案作为 decoder target 占位\n",
    "\n",
    "        batch_all_answers.append([ans.strip() for ans in sample['answers']])  # 保存该样本的所有参考答案列表\n",
    "\n",
    "    tensor_data = tokenizer(\n",
    "        batch_inputs,\n",
    "        text_target=batch_targets,\n",
    "        padding=True,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    tensor_data['labels'][tensor_data['labels'] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # 返回字典，区分 tensor 和 list 数据\n",
    "    return {\n",
    "        'tensor_data': tensor_data,       # 包含 input_ids, attention_mask, labels\n",
    "        'list_data': {\n",
    "            'all_answers': batch_all_answers\n",
    "        }\n",
    "    }\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_data,                     # 验证数据集\n",
    "    batch_size=valid_batch_size,    \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a355ce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "batch shape: {'input_ids': torch.Size([8, 487]), 'attention_mask': torch.Size([8, 487]), 'labels': torch.Size([8, 7])}\n",
      "{'input_ids': tensor([[ 1616, 15223,   523,  ...,     0,     0,     0],\n",
      "        [12203,    24,  6374,  ...,     0,     0,     0],\n",
      "        [ 1944,   164,  7424,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 2747,  1263,  2886,  ...,     0,     0,     0],\n",
      "        [10361,   789,   164,  ...,     0,     0,     0],\n",
      "        [  623,   500,   956,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[32127,   173,  1752, 32126,     1,  -100,  -100],\n",
      "        [32127,  6736,   670, 32126,     1,  -100,  -100],\n",
      "        [32127,  1026,  1236,   547, 32126,     1,  -100],\n",
      "        [32127,  2747, 20301, 24860,   456, 32126,     1],\n",
      "        [32127,   100,    38,   108,  6670, 32126,     1],\n",
      "        [32127,  2747, 17851,  2779, 32126,     1,  -100],\n",
      "        [32127,  1012,   301,   159, 32126,     1,  -100],\n",
      "        [32127,  1586, 32126,     1,  -100,  -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "# Check the batch data\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print('batch shape:', {k: v.shape for k, v in batch.items()})\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f947a",
   "metadata": {},
   "source": [
    "# 2. 训练模型\n",
    "## 2.1. 构建模型\n",
    "直接使用 Transformers 库自带的 AutoModelForSeq2SeqLM 函数来构建模型\n",
    "- checkpoint 已经包含 encoder + decoder + lm_head\n",
    "- 可以直接调用 forward 训练，或者 generate 生成文本\n",
    "- 不需要自己再加 lm_head 或写自定义 forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8e22a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59763457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/anaconda3/envs/t5/lib/python3.11/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6481b",
   "metadata": {},
   "source": [
    "## 2.2. Train Loop\n",
    "使用 AutoModelForSeq2SeqLM 构造的模型已经封装好了对应的损失函数，并且计算出的损失会直接包含在模型的输出 outputs 中，可以直接通过 outputs.loss 获得，因此训练循环为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8744e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch}', ncols=100)\n",
    "\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = {k: v.to(device) for k, v in batch_data.items() if isinstance(v, torch.Tensor)}\n",
    "                \n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (finish_batch_num + batch)\n",
    "        progress_bar.set_description(f'loss: {avg_loss:>7f}')\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        wandb.log({\"train/loss\": avg_loss}, step=finish_batch_num + batch)\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8938f507",
   "metadata": {},
   "source": [
    "## 2.3. Test Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0375f2a",
   "metadata": {},
   "source": [
    "### 解码\n",
    "> 在本文中，我们使用 AutoModelForSeq2SeqLM 模型自带的 generate() 函数，通过柱搜索 (Beam search) 解码出翻译结果（使用模型默认解码参数）。\n",
    "> \n",
    "- `generate()` 使用 AutoModelForSeq2SeqLM 构造的模型同样对 Decoder 的解码过程进行了封装，我们只需要调用模型的 generate() 函数就可以自动地逐个生成预测 token\n",
    "\n",
    "- `tokenizer.batch_decode()` 在 generate() 生成 token ID 之后，我们通过分词器自带的 tokenizer.batch_decode() 函数将 batch 中所有的 token ID 序列都转换为文本\n",
    "\n",
    "### jieba\n",
    "- ❌ 如果用`bleu = BLUE(tokenize = 'zh')`，英文分词按照字母切\n",
    "- ✅ jieba.cut(text.strip()) 用 jieba 对中文进行分词，英文和数字默认按字符切\n",
    "\n",
    "### BLEU\n",
    "- 计算 BLEU-1~4\n",
    "  - ❌ `sacrebleu` 默认标准 BLEU-4\n",
    "  - ✅ `nltk.translate.bleu_score` 可自定义权重\n",
    "\n",
    "- weights 决定 n-gram 权重：\n",
    "  - BLEU-1：只计算 unigram\n",
    "  - BLEU-2：计算 unigram + bigram\n",
    "  - BLEU-3：计算 1~3 gram\n",
    "  - BLEU-4：标准 BLEU-4\n",
    "\n",
    "- `corpus_bleu` 会对整个 corpus 做平均，使用 list of list 自动选择最匹配的label\n",
    "\n",
    "> 问题：BLEU对短文本效果不好\n",
    "> BLEU 依赖于词序的精确 N-gram 重叠。但在简短回答或摘要任务中：\n",
    "> 1. 答案多样性高： 一个问题的正确答案可能有多种表达方式，即使语义完全一致，词序和词汇也可能不同。\n",
    "> 2. 长度短： 简短回答通常很短，导致 4-gram 匹配难度极大，BLEU 分数容易不合理地降为 0。\n",
    "> 3. 强调召回率： 简短回答更看重关键信息的覆盖率（即信息是否都答到了），而 BLEU 本身是一个精度指标，对漏掉关键信息惩罚不足。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abd051a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zh_tokenize(text):\n",
    "    \"\"\"中文 + 混合英文分词\"\"\"\n",
    "    return list(jieba.cut(text.strip())) if text.strip() else ['empty']\n",
    "\n",
    "def test_loop(dataloader, model, mode='valid', debug_sample_num=10):\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch, batch_data in enumerate(tqdm(dataloader, desc=f\"Evaluating {mode}\")):\n",
    "        # tensor 类型的数据（input_ids、labels等） 和 list 类型的数据（all_answers）分开处理\n",
    "        batch_data_tensor = {k: v.to(device) for k, v in batch_data['tensor_data'].items() if isinstance(v, torch.Tensor)}\n",
    "        batch_data_list = batch_data['list_data']\n",
    "        \n",
    "        # 生成预测\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                input_ids=batch_data_tensor[\"input_ids\"],\n",
    "                attention_mask=batch_data_tensor[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=beam_size,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            )\n",
    "        \n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)  # batch_decode 把 token id 转成文本字符串\n",
    "\n",
    "        # 遍历每条样本\n",
    "        for i, pred_text in enumerate(decoded_preds):\n",
    "            pred_tokens = zh_tokenize(pred_text)\n",
    "            label_tokens = [zh_tokenize(r) for r in batch_data_list['all_answers'][i]]  # list of list\n",
    "            preds.append(pred_tokens)   # preds：存放模型预测的 token 列表（每条样本一个 list）\n",
    "            labels.append(label_tokens) # labels：存放该预测对应的 所有参考答案 token 列表（list of list）\n",
    "\n",
    "    # 随机打印debug_sample_num个样本\n",
    "    if debug_sample_num > 0:\n",
    "        sample_indices = random.sample(range(len(preds)), min(debug_sample_num, len(preds)))\n",
    "        print(\"\\nRandom debug samples:\")\n",
    "        for idx in sample_indices:\n",
    "            print(f\"PRED: {preds[idx]} REF:  {labels[idx]}\")\n",
    "\n",
    "    # 计算 BLEU-1 ~ BLEU-4\n",
    "    P1 = corpus_bleu(labels, preds, weights=(1,0,0,0))\n",
    "    P2 = corpus_bleu(labels, preds, weights=(0.5,0.5,0,0))\n",
    "    P3 = corpus_bleu(labels, preds, weights=(0.33,0.33,0.33,0))\n",
    "    P4 = corpus_bleu(labels, preds, weights=(0.25,0.25,0.25,0.25))\n",
    "\n",
    "    class Result:\n",
    "        pass\n",
    "\n",
    "    result = Result()\n",
    "    result.score = P4 * 100\n",
    "    result.precisions = (P1 * 100, P2 * 100, P3 * 100, P4 * 100)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa359a35",
   "metadata": {},
   "source": [
    "## 2.4. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "512d5cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcocora14\u001b[0m (\u001b[33mcocora14-the-university-of-sydney\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/med/wandb/run-20250928_053028-isgvqwri</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation/runs/isgvqwri' target=\"_blank\">t5_train</a></strong> to <a href='https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation' target=\"_blank\">https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation/runs/isgvqwri' target=\"_blank\">https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation/runs/isgvqwri</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"text2text_generation\",\n",
    "    name=\"t5_train\",\n",
    ")\n",
    "wandb.config.update({\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": epoch_num,\n",
    "    \"batch_size\": train_batch_size,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a5469",
   "metadata": {},
   "source": [
    "### 优化器\n",
    "使用 AdamW 优化器，适合 Transformer 类模型。\n",
    "- model.parameters()：告诉优化器哪些参数需要更新。\n",
    "- lr=learning_rate：学习率。\n",
    "- 作用：控制参数更新策略（带权重衰减的 Adam）。\n",
    "\n",
    "### Accelerator 封装\n",
    "```bash\n",
    "(t5) med@server:~$ nvidia-smi\n",
    "Sun Sep 28 01:46:54 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:18:00.0 Off |                  N/A |\n",
    "| 49%   84C    P2             239W / 350W |  15720MiB / 24576MiB |     96%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA GeForce RTX 3090        Off | 00000000:3B:00.0 Off |                  N/A |\n",
    "| 30%   33C    P8              11W / 350W |      3MiB / 24576MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   2  NVIDIA GeForce RTX 3090        Off | 00000000:5E:00.0 Off |                  N/A |\n",
    "|  0%   27C    P8               9W / 370W |      3MiB / 24576MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   3  NVIDIA GeForce RTX 3090        Off | 00000000:86:00.0 Off |                  N/A |\n",
    "| 30%   28C    P8              10W / 350W |      3MiB / 24576MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A    179783      C   ...ed/anaconda3/envs/t5/bin/python3.11    15714MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "### 学习率调度器\n",
    "使用 线性衰减学习率：从初始 lr 线性下降到 0。\n",
    "\n",
    "- 参数：\n",
    "  - num_warmup_steps=0 → 没有预热阶段\n",
    "  - num_training_steps=epoch_num * len(train_dataloader) → 总训练步数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "959a126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/anaconda3/envs/t5/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.945684: 100%|███████████████████████████████████████████| 1634/1634 [07:26<00:00,  3.66it/s]\n",
      "Evaluating valid:   0%|                                  | 0/88 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.598 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:32<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random debug samples:\n",
      "PRED: ['3788', '-', '5588', '元'] REF:  [['破', '亿'], ['首日', '销量', '过', '亿']]\n",
      "PRED: ['50', '个'] REF:  [['50'], ['50', '个']]\n",
      "PRED: ['阜阳', '友好', '医院'] REF:  [['阜阳', '友好', '医院']]\n",
      "PRED: ['90', '年代'] REF:  [['20', '世纪', '90', '年代']]\n",
      "PRED: ['95543'] REF:  [['95543']]\n",
      "PRED: ['神经科'] REF:  [['神经科']]\n",
      "PRED: ['3', '个', '月'] REF:  [['3', '个', '月']]\n",
      "PRED: ['一周', '多'] REF:  [['一周', '多', '时间']]\n",
      "PRED: ['二本'] REF:  [['二本']]\n",
      "PRED: ['138.3', 'x67.1', 'mm'] REF:  [['138.3', 'x67.1', 'x7.1', 'mm']]\n",
      "Validation BLEU scores: 49.90\n",
      "BLEU-1: 76.00, BLEU-2: 69.36, BLEU-3: 60.28, BLEU-4: 49.90\n",
      "saving new weights...\n",
      "\n",
      "Model weights saved to: /home/med/selflearning/stage1-t5/model/bin/epoch_1_valid_BLEU_49.90_model_weights.bin\n",
      "\n",
      "Epoch 2/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.656441: 100%|███████████████████████████████████████████| 1634/1634 [07:51<00:00,  3.46it/s]\n",
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:34<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random debug samples:\n",
      "PRED: ['我', '想'] REF:  [['我', '想']]\n",
      "PRED: ['飞利浦'] REF:  [['飞利浦']]\n",
      "PRED: ['放风', '草'] REF:  [['放风', '草'], ['防风', '草']]\n",
      "PRED: ['三四根'] REF:  [['两根']]\n",
      "PRED: ['几千', '到', '上万'] REF:  [['几千', '到', '上万', '不', '等'], ['几千', '到', '上万']]\n",
      "PRED: ['倍', '浓', '植物', '奶牛', '系列'] REF:  [['倍', '浓', '植物', '奶牛', '系列']]\n",
      "PRED: ['五一', '至', '十一', '之间'] REF:  [['五一', '至', '十一', '之间'], ['五一', '至', '十一']]\n",
      "PRED: ['两到', '三年'] REF:  [['两到', '三年']]\n",
      "PRED: ['白起'] REF:  [['白起']]\n",
      "PRED: ['一周', '多'] REF:  [['一周', '多', '时间']]\n",
      "Validation BLEU scores: 52.75\n",
      "BLEU-1: 79.15, BLEU-2: 72.33, BLEU-3: 63.15, BLEU-4: 52.75\n",
      "saving new weights...\n",
      "\n",
      "Model weights saved to: /home/med/selflearning/stage1-t5/model/bin/epoch_2_valid_BLEU_52.75_model_weights.bin\n",
      "\n",
      "Epoch 3/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.520224: 100%|███████████████████████████████████████████| 1634/1634 [07:53<00:00,  3.45it/s]\n",
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:33<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random debug samples:\n",
      "PRED: ['90', '-', '140', '/', '60', '-', '90mmhg'] REF:  [['90', '-', '140', '/', '60', '-', '90mmhg', '之间'], ['90', '-', '140', '/', '60', '-', '90mmhg']]\n",
      "PRED: ['半年', '后'] REF:  [['半年', '后']]\n",
      "PRED: ['二十五', '到', '三十分钟'] REF:  [['差不多', '二十五', '到', '三十分钟'], ['二十五', '到', '三十分钟']]\n",
      "PRED: ['安化', '金花', '黑茶'] REF:  [['安化', '金花', '黑茶']]\n",
      "PRED: ['新', '工匠', '镇'] REF:  [['新', '工匠', '镇']]\n",
      "PRED: ['MIUI', '+', '原生', 'Android'] REF:  [['MIUI', '+', '原生', 'Android', '(', '基于', 'Android', ' ', 'OS', ' ', 'v2.3', '.', '5', ')', '双系统'], ['MIUI', '操作系统']]\n",
      "PRED: ['不能', '超过', '10%'] REF:  [['地面', '的', '不能', '超过', '10%']]\n",
      "PRED: ['90', '/', '60', '-', '139', '/', '89'] REF:  [['90', '/', '60', '-', '139', '/', '89']]\n",
      "PRED: ['12', '月', '31', '日', '24', ':', '00', '点'] REF:  [['12', '月', '31', '日', '24', ':', '00']]\n",
      "PRED: ['5000', '元', '左右'] REF:  [['平均工资', '约', '5000', '元', '左右'], ['平均工资', '约', '5000', '元']]\n",
      "Validation BLEU scores: 54.42\n",
      "BLEU-1: 79.79, BLEU-2: 73.50, BLEU-3: 64.73, BLEU-4: 54.42\n",
      "saving new weights...\n",
      "\n",
      "Model weights saved to: /home/med/selflearning/stage1-t5/model/bin/epoch_3_valid_BLEU_54.42_model_weights.bin\n",
      "\n",
      "Epoch 4/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.435098: 100%|███████████████████████████████████████████| 1634/1634 [07:46<00:00,  3.51it/s]\n",
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:34<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random debug samples:\n",
      "PRED: ['约', '5000', '元', '左右'] REF:  [['平均工资', '约', '5000', '元', '左右'], ['平均工资', '约', '5000', '元']]\n",
      "PRED: ['170cm'] REF:  [['170cm']]\n",
      "PRED: ['1000', '-', '1500', '元', '左右'] REF:  [['1000', '-', '1500', '元'], ['1000', '-', '1500', '元', '左右']]\n",
      "PRED: ['二本'] REF:  [['2', '本'], ['二本']]\n",
      "PRED: ['U', '系列'] REF:  [['U', '系列']]\n",
      "PRED: ['密歇根州'] REF:  [['密歇根州'], ['密歇根州', '（', 'Michigan', '）'], ['Michigan']]\n",
      "PRED: ['2.7', '亿美元'] REF:  [['2.7', '亿美元']]\n",
      "PRED: ['每周六', '晚上', '8', '点', '-', '9', '点', '9', '点'] REF:  [['每周六', ' ', '周日', '晚上', '8', '点', '-', '9', '点']]\n",
      "PRED: ['罗琦'] REF:  [['黄小琥'], ['应该', '是', '黄小琥']]\n",
      "PRED: ['2017', '-', '05', '-', '02'] REF:  [['下周']]\n",
      "Validation BLEU scores: 53.87\n",
      "BLEU-1: 79.56, BLEU-2: 72.91, BLEU-3: 64.02, BLEU-4: 53.87\n",
      "No improvement, patience_counter: 1/3\n",
      "\n",
      "Epoch 5/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.375054: 100%|███████████████████████████████████████████| 1634/1634 [07:56<00:00,  3.43it/s]\n",
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:36<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random debug samples:\n",
      "PRED: ['3%'] REF:  [['3%']]\n",
      "PRED: ['4', '万元', '左右'] REF:  [['4', ' ', '万元'], ['4', ' ', '万元', '左右']]\n",
      "PRED: ['85.5', '*', '54'] REF:  [['85.5', '*', '54']]\n",
      "PRED: ['2', '时', '30', '分至', '5', '时', '50', '分'] REF:  [['2', '时', '30', '分']]\n",
      "PRED: ['属鸡'] REF:  [['鸡']]\n",
      "PRED: ['15000', '-', '200000IU', '/', 'L'] REF:  [['15000', '-', '200000IU', '/', 'L']]\n",
      "PRED: ['14pt'] REF:  [['12pt']]\n",
      "PRED: ['新', '工匠', '镇'] REF:  [['新', '工匠', '镇']]\n",
      "PRED: ['巾'] REF:  [['巾']]\n",
      "PRED: ['农历', '六月', '二十四日'] REF:  [['农历', '六月', '二十四日']]\n",
      "Validation BLEU scores: 55.91\n",
      "BLEU-1: 81.07, BLEU-2: 74.57, BLEU-3: 65.81, BLEU-4: 55.91\n",
      "saving new weights...\n",
      "\n",
      "Model weights saved to: /home/med/selflearning/stage1-t5/model/bin/epoch_5_valid_BLEU_55.91_model_weights.bin\n",
      "\n",
      "Epoch 6/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.329643: 100%|███████████████████████████████████████████| 1634/1634 [07:52<00:00,  3.46it/s]\n",
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:32<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random debug samples:\n",
      "PRED: ['天津', '阳光', '男科', '医院'] REF:  [['天津', '阳光', '男科', '医院']]\n",
      "PRED: ['4', '～', '6', '周'] REF:  [['早期', '梅毒', '4', '～', '6', '周']]\n",
      "PRED: ['16', '岁'] REF:  [['16', '岁']]\n",
      "PRED: ['美利达', '公爵', '600'] REF:  [['美利达', '公爵', '600']]\n",
      "PRED: ['黄小琥'] REF:  [['黄小琥'], ['应该', '是', '黄小琥']]\n",
      "PRED: ['两三天', '之后'] REF:  [['两三天', '之后']]\n",
      "PRED: ['你们'] REF:  [['你', '的', '朋友', '们'], ['你们', '的', '这些', '家伙']]\n",
      "PRED: ['1921', '年', '7', '月'] REF:  [['1921', '年', '7', '月']]\n",
      "PRED: ['80', '厘米'] REF:  [['80', '厘米'], ['80', '（', '厘米', '）'], ['80']]\n",
      "PRED: ['40', '-', '60', '度', '左右', '洁净', '的', '温水'] REF:  [['40', '-', '60', '度', '左右'], ['40', '-', '60', '度']]\n",
      "Validation BLEU scores: 53.90\n",
      "BLEU-1: 79.92, BLEU-2: 73.21, BLEU-3: 64.14, BLEU-4: 53.90\n",
      "No improvement, patience_counter: 1/3\n",
      "\n",
      "Epoch 7/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.294509: 100%|███████████████████████████████████████████| 1634/1634 [07:52<00:00,  3.46it/s]\n",
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:32<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random debug samples:\n",
      "PRED: ['200', '元到', '1000', '元', '每平米'] REF:  [['大概', '在', '200', '元到', '1000', '元'], ['200', '元到', '1000', '元']]\n",
      "PRED: ['PE', '材料', '加', '保鲜膜', '复合', '而成'] REF:  [['XPE']]\n",
      "PRED: ['广州', '东方', '英文', '书院'] REF:  [['广州', '东方', '英文', '书院']]\n",
      "PRED: ['无锡', '嘉仕', '恒信', '医院'] REF:  [['无锡', '嘉仕', '恒信', '医院']]\n",
      "PRED: ['属鸡'] REF:  [['鸡']]\n",
      "PRED: ['55', '公斤'] REF:  [['55', '公斤']]\n",
      "PRED: ['2016', '年', '9', '月', '10', '日至', '12', '日'] REF:  [['2016', '年', '9', '月', '10', '日至', '12', '日']]\n",
      "PRED: ['750', '分'] REF:  [['750', '分']]\n",
      "PRED: ['140MB', '/', 's'] REF:  [['闪迪酷', '豆', 'USB3.0'], ['闪迪', 'CZ43', '酷豆', '系列'], ['闪迪', 'CZ43', '酷豆', 'USB3.0'], ['闪迪']]\n",
      "PRED: ['MIUI', '/', 'notes'] REF:  [['MIUI', '/', 'notes', '文件夹']]\n",
      "Validation BLEU scores: 53.77\n",
      "BLEU-1: 79.96, BLEU-2: 73.33, BLEU-3: 64.16, BLEU-4: 53.77\n",
      "No improvement, patience_counter: 2/3\n",
      "\n",
      "Epoch 8/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.266385: 100%|███████████████████████████████████████████| 1634/1634 [07:55<00:00,  3.43it/s]\n",
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:33<00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random debug samples:\n",
      "PRED: ['长', '64cm', '×', '宽', '41cm'] REF:  [['长', '64cm', '×', '宽', '41cm', '×', '厚', '26cm']]\n",
      "PRED: ['十天'] REF:  [['一般', '十天'], ['十天']]\n",
      "PRED: ['1.48', '吨'] REF:  [['约', '为', '1.35', '-', '1.45', '吨'], ['1.35', '-', '1.45', '吨'], ['1.35', '-', '1.45']]\n",
      "PRED: ['华康', '利爽'] REF:  [['华康', '利爽'], ['左', '氧氟沙星']]\n",
      "PRED: ['宋欣佳怡'] REF:  [['宋欣佳怡']]\n",
      "PRED: ['666.67', '平方米'] REF:  [['30000.15', '平方米']]\n",
      "PRED: ['紫背', '菜'] REF:  [['紫背', '菜']]\n",
      "PRED: ['122'] REF:  [['122']]\n",
      "PRED: ['平安', '智慧', '星', '少儿', '万能', '险'] REF:  [['平安', '智慧', '星', '少儿', '万能', '险']]\n",
      "PRED: ['今年', '中旬'] REF:  [['今年', '中旬']]\n",
      "Validation BLEU scores: 53.56\n",
      "BLEU-1: 79.65, BLEU-2: 73.11, BLEU-3: 63.85, BLEU-4: 53.56\n",
      "No improvement, patience_counter: 3/3\n",
      "\n",
      "Early stopping triggered at epoch 8.\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用 AdamW 优化器\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Accelerator 封装\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, valid_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, valid_dataloader\n",
    ")\n",
    "\n",
    "# 学习率调度器\n",
    "num_training_steps = epoch_num * len(train_dataloader)\n",
    "num_warmup_steps = int(0.05 * num_training_steps)  # 前 5% steps 做 warmup\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# 初始化指标\n",
    "total_loss = 0.\n",
    "best_bleu = 0.\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "# 训练 + 验证循环\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_bleu = test_loop(valid_dataloader, model, mode='valid', debug_sample_num=10)\n",
    "\n",
    "    print(f\"Validation BLEU scores: {valid_bleu.score:.2f}\")\n",
    "    P1, P2, P3, P4 = valid_bleu.precisions\n",
    "    print(f\"BLEU-1: {P1:.2f}, BLEU-2: {P2:.2f}, BLEU-3: {P3:.2f}, BLEU-4: {P4:.2f}\")\n",
    "\n",
    "    # 判断是否保存最优模型\n",
    "    if valid_bleu.score > best_bleu:\n",
    "        best_bleu = valid_bleu.score\n",
    "        patience_counter = 0  # 重置耐心计数器\n",
    "        print('saving new weights...\\n')\n",
    "        model_path = os.path.join(save_dir, f\"epoch_{t+1}_valid_BLEU_{valid_bleu.score:0.2f}_model_weights.bin\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Model weights saved to: {model_path}\\n\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement, patience_counter: {patience_counter}/{patience}\\n\")\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {t+1}.\")\n",
    "            break\n",
    "    \n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0f34a",
   "metadata": {},
   "source": [
    "### 训练数据\n",
    "[wandb](https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation/runs/v9s5tgxa?nw=nwusercocora14)\n",
    "\n",
    "| Epoch | Loss     | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | Notes                    |\n",
    "| ----- | -------- | ------ | ------ | ------ | ------ | ------------------------ |\n",
    "| 1     | 0.945684 | 76.00  | 69.36  | 60.28  | 49.90  | save                     |\n",
    "| 2     | 0.656441 | 79.15  | 72.33  | 63.15  | 52.75  | save                     |\n",
    "| 3     | 0.520224 | 79.79  | 73.50  | 64.73  | 54.42  | save                     |\n",
    "| 4     | 0.435098 | 79.56  | 72.91  | 64.02  | 53.87  | -                        |\n",
    "| 5     | 0.375054 | 81.07  | 74.57  | 65.81  | 55.91  | save                     |\n",
    "| 6     | 0.329643 | 79.92  | 73.21  | 64.14  | 53.90  | -                        |\n",
    "| 7     | 0.294509 | 79.96  | 73.33  | 64.16  | 53.77  | -                        |\n",
    "| 8     | 0.266385 | 79.65  | 73.11  | 63.85  | 53.56  | Early stop |\n",
    "\n",
    "\n",
    "![loss](t5-base/train/Snipaste_2025-09-28_03-18-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a18160",
   "metadata": {},
   "source": [
    "# 3. Test Model\n",
    "训练完成后，我们加载在验证集上性能最优的模型权重，汇报其在测试集上的性能，并且将模型的预测结果保存到文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74b64b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, output_path):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    sources, preds, labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print('evaluating on test set...')\n",
    "        for batch_data in tqdm(test_dataloader):\n",
    "            batch_data = {k: v.to(device) for k, v in batch_data.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=beam_size,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            )\n",
    "\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "\n",
    "            # 转 numpy\n",
    "            generated_tokens = generated_tokens.cpu().numpy()\n",
    "            label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "            label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "\n",
    "            # 解码\n",
    "            decoded_sources = tokenizer.batch_decode(\n",
    "                batch_data[\"input_ids\"].cpu().numpy(),\n",
    "                skip_special_tokens=True,\n",
    "                use_source_tokenizer=True\n",
    "            )\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "            # 分词 & 保存\n",
    "            sources += [s.strip() for s in decoded_sources]\n",
    "            preds += [zh_tokenize(p.strip()) if p.strip() else ['empty'] for p in decoded_preds]\n",
    "            labels += [[zh_tokenize(l.strip())] if l.strip() else [['empty']] for l in decoded_labels]\n",
    "\n",
    "    P1 = corpus_bleu(labels, preds, weights=(1,0,0,0))\n",
    "    P2 = corpus_bleu(labels, preds, weights=(0.5,0.5,0,0))\n",
    "    P3 = corpus_bleu(labels, preds, weights=(0.33,0.33,0.33,0))\n",
    "    P4 = corpus_bleu(labels, preds, weights=(0.25,0.25,0.25,0.25))\n",
    "\n",
    "    print(f\"Test BLEU-1: {P1*100:.2f}, BLEU-2: {P2*100:.2f}, BLEU-3: {P3*100:.2f}, BLEU-4: {P4*100:.2f}\")\n",
    "\n",
    "    results = []\n",
    "    print('saving predicted results...')\n",
    "    for source, pred, label in zip(sources, preds, labels):\n",
    "        results.append({\n",
    "            \"sentence\": source, \n",
    "            \"prediction\": pred, \n",
    "            \"translation\": label \n",
    "        })\n",
    "\n",
    "    with open(output_path, 'wt', encoding='utf-8') as f:\n",
    "        for example_result in results:\n",
    "            f.write(json.dumps(example_result, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3a5144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_181892/388319212.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/med/selflearning/stage1-t5/model/mengzi-t5-base/pytorch_model.bin'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 182/182 [03:09<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU-1: 6.76, BLEU-2: 4.51, BLEU-3: 3.07, BLEU-4: 1.94\n",
      "saving predicted results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test before training\n",
    "output_path = f'{output_dir}/test_before_training.json'\n",
    "model.load_state_dict(torch.load('/home/med/selflearning/stage1-t5/model/mengzi-t5-base/pytorch_model.bin'))\n",
    "test_model(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed67cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_181892/4125827008.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 182/182 [02:53<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU-1: 76.61, BLEU-2: 67.71, BLEU-3: 57.07, BLEU-4: 45.76\n",
      "saving predicted results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test after training\n",
    "output_path = f'{output_dir}/test_after_training.json'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "test_model(model, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219404c9",
   "metadata": {},
   "source": [
    "### 测试结果\n",
    "经过微调，模型在测试集上的 BLEU 值从 1.69 上升到 49.49，证明了我们对模型的微调是成功的。\n",
    "\n",
    "训练前：Test BLEU-1: 6.76, BLEU-2: 4.51, BLEU-3: 3.07, BLEU-4: 1.94\n",
    "\n",
    "训练后：Test BLEU-1: 76.61, BLEU-2: 67.71, BLEU-3: 57.07, BLEU-4: 45.76\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
