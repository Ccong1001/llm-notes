{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bf427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/anaconda3/envs/t5/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AdamW, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "# from sacrebleu.metrics import BLEU\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import jieba\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d08010",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dataset_size = 200000\n",
    "max_input_length = 512\n",
    "max_target_length = 32\n",
    "stride = 128\n",
    "train_batch_size = 8\n",
    "valid_batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "epoch_num = 10\n",
    "beam_size = 4\n",
    "no_repeat_ngram_size = 2\n",
    "\n",
    "seed = 5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "save_dir = \"/home/med/selflearning/stage1-t5/model/bin\"\n",
    "output_dir = '/home/med/selflearning/stage1-t5/data/DuReaderQG/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb71efb3",
   "metadata": {},
   "source": [
    "# 1. 准备数据\n",
    "## 1.1. 构建数据集\n",
    "### 数据集\n",
    "先编写继承自 Dataset 类的自定义数据集类用于组织样本和标签\n",
    "\n",
    "DuReaderQG: \n",
    "- Train data size: 14520\n",
    "- valid data size: 700\n",
    "\n",
    "### 合并answers\n",
    "训练集只有一个答案，验证集一个问题可能对应有多个参考答案，将相同question和context的所有answer合并进一个answers列表。\n",
    "\n",
    "验证集数据格式：\n",
    "\n",
    "`{\"context\": \"还没有最后确定，暂定2017年\", \"question\": \"余罪第三季开播时间\", \"answers\": [\"暂定2017年\", \"2017年\"], \"id\": 9}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f907ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "\n",
    "        with open(data_file, 'rt', encoding = 'utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                sample = json.loads(line.strip())\n",
    "                Data[idx] = sample\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_data = T5('/home/med/selflearning/stage1-t5/data/DuReaderQG/train.json')\n",
    "valid_data = T5('/home/med/selflearning/stage1-t5/data/DuReaderQG/dev_merged.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6043d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 14520\n",
      "valid data size: 700\n",
      "{'context': '第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。', 'answer': '第35集', 'question': '仙剑奇侠传3第几集上天界', 'id': 0}\n",
      "{'context': '年基准利率4.35%。 从实际看,贷款的基本条件是: 一是中国大陆居民,年龄在60岁以下; 二是有稳定的住址和工作或经营地点; 三是有稳定的收入来源; 四是无不良信用记录,贷款用途不能作为炒股,赌博等行为; 五是具有完全民事行为能力。', 'question': '2017年银行贷款基准利率', 'answers': ['年基准利率4.35%', '4.35%'], 'id': 1}\n"
     ]
    }
   ],
   "source": [
    "# Check data size and sample\n",
    "print(f'Train data size: {len(train_data)}')\n",
    "print(f'valid data size: {len(valid_data)}')\n",
    "\n",
    "print(next(iter(train_data)))\n",
    "print(next(iter(valid_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc4b22",
   "metadata": {},
   "source": [
    "## 1.2. 数据预处理\n",
    "### T5Tokenizer\n",
    "AutoTokenizer -> T5Tokenizer\n",
    "> 第五章：模型与分词器\n",
    "> \n",
    "> 调用 Tokenizer.save_pretrained() 函数会在保存路径下创建三个文件：\n",
    "> \n",
    "> - special_tokens_map.json：映射文件，里面包含 unknown token 等特殊字符的映射关系；\n",
    "> - tokenizer_config.json：分词器配置文件，存储构建分词器需要的参数；\n",
    "> - vocab.txt：词表，一行一个 token，行号就是对应的 token ID（从 0 开始）。\n",
    ">\n",
    "- ❌ mengzi使用modelscope pipeline，没有tokenizer.json，Autotokenizer加载失败\n",
    "- ✅ 明确使用 T5Tokenizer，避免 AutoTokenizer 转换失败"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c41e661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"/home/med/selflearning/stage1-t5/model/mengzi-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e311626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overflowing_tokens': [], 'num_truncated_tokens': -34, 'input_ids': [1707, 1467, 992, 3979, 707, 100, 379, 645, 647, 9724, 1252, 1, 379, 2838, 647, 843, 408, 10694, 17973, 1763, 3, 1276, 87, 84, 2744, 84, 1419, 6141, 3, 122, 6409, 9, 2177, 17534, 5, 1707, 1468, 11725, 229, 3, 408, 5542, 119, 28060, 3, 18440, 3844, 4, 5542, 2190, 1468, 3, 54, 1069, 12914, 83, 5665, 335, 215, 9, 4514, 17339, 69, 4, 843, 408, 1200, 3771, 1902, 10, 3, 6625, 8435, 603, 1100, 3, 266, 119, 1954, 4, 5542, 145, 711, 27530, 11755, 3, 13737, 21, 304, 3779, 68, 843, 408, 5, 21769, 7074, 4, 122, 6409, 5184, 299, 854, 5, 21769, 3, 299, 854, 720, 267, 448, 756, 58, 807, 2037, 87, 1252, 481, 15, 2125, 4, 122, 6409, 3412, 1707, 1468, 3, 5542, 559, 6861, 532, 478, 3, 978, 87, 1252, 9948, 4, 5542, 1904, 39, 4380, 144, 3, 122, 6409, 2078, 3, 1990, 1252, 9, 87, 1252, 9694, 4, 153, 1990, 1252, 446, 137, 260, 1990, 73, 2088, 3, 12287, 2190, 87, 4, 5542, 229, 1990, 1252, 5903, 3, 4848, 578, 39, 13026, 20402, 1877, 3, 57, 2279, 604, 446, 4, 602, 169, 27231, 277, 10581, 10992, 481, 76, 546, 290, 4, 887, 1276, 87, 5764, 14496, 975, 25035, 3, 6120, 286, 866, 5, 10992, 3, 1202, 338, 27, 10992, 1456, 2822, 584, 4, 827, 8908, 8, 478, 3, 481, 60, 18651, 1877, 334, 4, 5542, 6184, 10236, 53, 3, 7773, 277, 1990, 1252, 25033, 4, 1276, 87, 9, 1801, 1990, 1037, 690, 565, 3, 11008, 9, 1990, 2431, 866, 396, 2003, 3, 1801, 1990, 12079, 3, 176, 6453, 4, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['仙', '剑', '奇', '侠', '传', '3', '第', '几', '集', '上天', '界', '</s>', '第', '35', '集', '雪', '见', '缓缓', '张开', '眼睛', ',', '景', '天', '又', '惊', '又', '喜', '之际', ',', '长', '卿', '和', '紫', '萱', '的', '仙', '船', '驶', '至', ',', '见', '众人', '无', '恙', ',', '也十分', '高兴', '。', '众人', '登', '船', ',', '用', '尽', '合力', '把', '自身的', '真', '气', '和', '水分', '输给', '她', '。', '雪', '见', '终于', '醒', '过来', '了', ',', '但却', '一脸', '木', '然', ',', '全', '无', '反应', '。', '众人', '向', '常', '胤', '求助', ',', '却发现', '人', '世界', '竟', '没有', '雪', '见', '的', '身世', '纪录', '。', '长', '卿', '询问', '清', '微', '的', '身世', ',', '清', '微', '语', '带', '双', '关', '说', '一切', '上了', '天', '界', '便', '有', '答案', '。', '长', '卿', '驾驶', '仙', '船', ',', '众人', '决定', '立马', '动', '身', ',', '往', '天', '界', '而去', '。', '众人', '来到', '一', '荒', '山', ',', '长', '卿', '指出', ',', '魔', '界', '和', '天', '界', '相连', '。', '由', '魔', '界', '进入', '通过', '神', '魔', '之', '井', ',', '便可', '登', '天', '。', '众人', '至', '魔', '界', '入口', ',', '仿', '若', '一', '黑色的', '蝙蝠', '洞', ',', '但', '始终', '无法', '进入', '。', '后来', '花', '楹', '发现', '只要有', '翅膀', '便', '能', '飞', '入', '。', '于是', '景', '天', '等人', '打下', '许多', '乌鸦', ',', '模仿', '重', '楼', '的', '翅膀', ',', '制作', '数', '对', '翅膀', '状', '巨', '物', '。', '刚', '佩戴', '在', '身', ',', '便', '被', '吸入', '洞', '口', '。', '众人', '摔', '落在', '地', ',', '抬头', '发现', '魔', '界', '守卫', '。', '景', '天', '和', '众', '魔', '套', '交', '情', ',', '自称', '和', '魔', '尊重', '楼', '相', '熟', ',', '众', '魔', '不理', ',', '打', '了起来', '。', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Check the tokenization result\n",
    "sample = train_data[0]\n",
    "context = sample['context']\n",
    "question = sample['question']\n",
    "answer = sample['answer']\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=300,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=50,\n",
    ")\n",
    "\n",
    "print(inputs)\n",
    "print(tokenizer.convert_ids_to_tokens(inputs.input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb43787",
   "metadata": {},
   "source": [
    "### T5训练模板\n",
    "> 第四章：开箱即用的 pipelines / 这些 pipeline 背后做了什么？\n",
    "> 1. 预处理 (preprocessing)，将原始文本转换为模型可以接受的输入格式；\n",
    "> 2. 将处理好的输入送入模型；\n",
    "> 3. 对模型的输出进行后处理 (postprocessing)，将其转换为人类方便阅读的格式\n",
    ">\n",
    "\n",
    "- inputs: `question <extra_id_0> context`\n",
    "- targets: `<extra_id_0> answer <extra_id_1>`\n",
    "\n",
    "### collate_fn\n",
    "> 直接使用 Transformers 库自带的 AutoModelForSeq2SeqLM 函数来构建模型，因此我们需要将每一个 batch 中的数据处理为该模型可接受的格式：一个包含 'input_ids'、'attention_mask'、'labels' 和 'decoder_input_ids' 键的字典。\n",
    "\n",
    "> 与我们之前任务中使用的纯 Encoder 模型不同，Seq2Seq 任务对应的模型采用的是 Encoder-Decoder 框架：Encoder 负责编码输入序列，Decoder 负责循环地逐个生成输出 token。因此，对于每一个样本，我们还需要额外准备 decoder input IDs 作为 Decoder 的输入。decoder input IDs 是标签序列的移位，在序列的开始位置增加了一个特殊的“序列起始符”。\n",
    ">\n",
    "\n",
    "这里用了 `text_target=`，它会在返回的 `batch_data` 里 自动生成：\n",
    "- `input_ids`（来自 `batch_inputs`）\n",
    "- `labels`（来自 `text_target`，已经右移处理）\n",
    "训练时，`T5ForConditionalGeneration` 的 `forward()` 会根据 `labels` 自动构造 `decoder_input_ids`（内部调用 `shift_tokens_right`）。不用手动传 `decoder_input_ids`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63891c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['question'].strip() + ' <extra_id_0> ' + sample['context'].strip())\n",
    "        batch_targets.append('<extra_id_0> ' + sample['answer'].strip() + ' <extra_id_1>')\n",
    "\n",
    "    # print(batch_inputs)\n",
    "    # print(batch_targets)\n",
    "    \n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs,               # 作为 encoder 的输入文本列表\n",
    "        text_target=batch_targets,  # 作为 decoder 的目标文本（transformers 的 T5 支持 text_target 直接生成 labels，已经右移处理）\n",
    "        padding=True,               # 对 batch 内样本进行 padding 到相同长度（默认 pad 到最长样本长度）\n",
    "        max_length=max_input_length,    # 超过该长度则截断\n",
    "        truncation=True,            # 允许截断\n",
    "        return_tensors=\"pt\"         # 返回 PyTorch 张量（dict 中包含 input_ids, attention_mask, labels 等）\n",
    "    )\n",
    "\n",
    "    batch_data['labels'][batch_data['labels'] == tokenizer.pad_token_id] = -100 # 将 labels 中等于 pad_token_id 的位置替换为 -100\n",
    "    \n",
    "    return batch_data               # 返回一个 dict（PyTorch 张量），可以直接被模型的 forward(**batch_data) 或 Trainer 使用。\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,                     # 训练数据集（实现了 __len__ 和 __getitem__）\n",
    "    batch_size=train_batch_size,    # 每个 batch 的样本数\n",
    "    shuffle=True,                   # 每个 epoch 前打乱数据（训练时常用）\n",
    "    collate_fn=collate_fn           # 自定义的 collate 函数（用于将多个样本合并为一个 batch）\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39ec22d",
   "metadata": {},
   "source": [
    "### collate_fn_valid\n",
    "- tensor_data：tensor 类型的数据（模型输入、labels）\n",
    "\n",
    "- list_data：list 类型的数据（多参考答案 all_answers）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "268692c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_valid(batch_samples):\n",
    "    \"\"\"\n",
    "    专门用于验证集的 collate_fn，处理多参考答案\n",
    "    \"\"\"\n",
    "    batch_inputs, batch_targets, batch_all_answers = [], [], []\n",
    "\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['question'].strip() + ' <extra_id_0> ' + sample['context'].strip())\n",
    "        batch_targets.append('<extra_id_0> ' + sample['answers'][0].strip() + ' <extra_id_1>')  # 用第一个答案作为 decoder target 占位\n",
    "\n",
    "        batch_all_answers.append([ans.strip() for ans in sample['answers']])  # 保存该样本的所有参考答案列表\n",
    "\n",
    "    tensor_data = tokenizer(\n",
    "        batch_inputs,\n",
    "        text_target=batch_targets,\n",
    "        padding=True,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    tensor_data['labels'][tensor_data['labels'] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # 返回字典，区分 tensor 和 list 数据\n",
    "    return {\n",
    "        'tensor_data': tensor_data,       # 包含 input_ids, attention_mask, labels\n",
    "        'list_data': {\n",
    "            'all_answers': batch_all_answers\n",
    "        }\n",
    "    }\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_data,                     # 验证数据集\n",
    "    batch_size=valid_batch_size,    \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a355ce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "batch shape: {'input_ids': torch.Size([8, 366]), 'attention_mask': torch.Size([8, 366]), 'labels': torch.Size([8, 12])}\n",
      "{'input_ids': tensor([[  789,  4671,  2119,  ...,     0,     0,     0],\n",
      "        [16500,   586,    50,  ...,     0,     0,     0],\n",
      "        [ 9856,   813, 15130,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  875,  5603,  9807,  ...,  1152, 10646,     1],\n",
      "        [   39,  7085, 27922,  ...,     0,     0,     0],\n",
      "        [  253,    39,  2421,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[32127,  5332, 21781, 32126,     1,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100],\n",
      "        [32127,  9373,  3680,   840, 32126,     1,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100],\n",
      "        [32127, 17190, 25972,  4582, 32126,     1,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100],\n",
      "        [32127,   655,  2772,   550,  2452,   920,  1176,   756,  2780,   636,\n",
      "         32126,     1],\n",
      "        [32127,   611, 32126,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100],\n",
      "        [32127,   253, 32126,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100],\n",
      "        [32127,  3502,   138,  1733,   165, 18767,   219,   370, 32126,     1,\n",
      "          -100,  -100],\n",
      "        [32127,  1258, 20557,   840, 32126,     1,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "# Check the batch data\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print('batch shape:', {k: v.shape for k, v in batch.items()})\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f947a",
   "metadata": {},
   "source": [
    "# 2. 训练模型\n",
    "## 2.1. 构建模型\n",
    "直接使用 Transformers 库自带的 AutoModelForSeq2SeqLM 函数来构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b8e22a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/anaconda3/envs/t5/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/med/anaconda3/envs/t5/lib/python3.11/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6481b",
   "metadata": {},
   "source": [
    "## 2.2. Train Loop\n",
    "使用 AutoModelForSeq2SeqLM 构造的模型已经封装好了对应的损失函数，并且计算出的损失会直接包含在模型的输出 outputs 中，可以直接通过 outputs.loss 获得，因此训练循环为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8744e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch}', ncols=100)\n",
    "\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = {k: v.to(device) for k, v in batch_data.items() if isinstance(v, torch.Tensor)}\n",
    "                \n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (finish_batch_num + batch)\n",
    "        progress_bar.set_description(f'loss: {avg_loss:>7f}')\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        wandb.log({\"train/loss\": avg_loss}, step=finish_batch_num + batch)\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8938f507",
   "metadata": {},
   "source": [
    "## 2.3. Test Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0375f2a",
   "metadata": {},
   "source": [
    "### 生成预测\n",
    "> 使用 AutoModelForSeq2SeqLM 构造的模型同样对 Decoder 的解码过程进行了封装，我们只需要调用模型的 generate() 函数就可以自动地逐个生成预测 token。\n",
    "> \n",
    "- `generate()` 使用 AutoModelForSeq2SeqLM 构造的模型同样对 Decoder 的解码过程进行了封装，我们只需要调用模型的 generate() 函数就可以自动地逐个生成预测 token\n",
    "\n",
    "- `tokenizer.batch_decode()` 在 generate() 生成 token ID 之后，我们通过分词器自带的 tokenizer.batch_decode() 函数将 batch 中所有的 token ID 序列都转换为文本\n",
    "\n",
    "### jieba\n",
    "- ❌ 如果用`bleu = BLUE(tokenize = 'zh')`，英文分词按照字母切\n",
    "- ✅ jieba.cut(text.strip()) 用 jieba 对中文进行分词，英文和数字默认按字符切\n",
    "\n",
    "### BLEU\n",
    "- 计算 BLEU-1~4\n",
    "  - ❌ `sacrebleu` 默认标准 BLEU-4\n",
    "  - ✅ `nltk.translate.bleu_score` 可自定义权重\n",
    "\n",
    "- weights 决定 n-gram 权重：\n",
    "  - BLEU-1：只计算 unigram\n",
    "  - BLEU-2：计算 unigram + bigram\n",
    "  - BLEU-3：计算 1~3 gram\n",
    "  - BLEU-4：标准 BLEU-4\n",
    "\n",
    "- `corpus_bleu` 会对整个 corpus 做平均，使用 list of list 自动选择最匹配的label\n",
    "\n",
    "> 问题：BLEU对短文本效果不好\n",
    "> BLEU 依赖于词序的精确 N-gram 重叠。但在简短回答或摘要任务中：\n",
    "> 1. 答案多样性高： 一个问题的正确答案可能有多种表达方式，即使语义完全一致，词序和词汇也可能不同。\n",
    "> 2. 长度短： 简短回答通常很短，导致 4-gram 匹配难度极大，BLEU 分数容易不合理地降为 0。\n",
    "> 3. 强调召回率： 简短回答更看重关键信息的覆盖率（即信息是否都答到了），而 BLEU 本身是一个精度指标，对漏掉关键信息惩罚不足。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd051a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zh_tokenize(text):\n",
    "    \"\"\"中文 + 混合英文分词\"\"\"\n",
    "    return list(jieba.cut(text.strip())) if text.strip() else ['empty']\n",
    "\n",
    "def test_loop(dataloader, model, mode='valid', debug_sample_num=10):\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=f\"Evaluating {mode}\")):\n",
    "        # tensor 类型的数据（input_ids、labels等） 和 list 类型的数据（all_answers）分开处理\n",
    "        batch_data_tensor = {k: v.to(device) for k, v in batch_data['tensor_data'].items() if isinstance(v, torch.Tensor)}\n",
    "        batch_data_list = batch_data['list_data']\n",
    "        \n",
    "        # 生成预测\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                input_ids=batch_data_tensor[\"input_ids\"],\n",
    "                attention_mask=batch_data_tensor[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=beam_size,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            )\n",
    "        \n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)  # batch_decode 把 token id 转成文本字符串\n",
    "\n",
    "        # 遍历每条样本\n",
    "        for i, pred_text in enumerate(decoded_preds):\n",
    "            pred_tokens = zh_tokenize(pred_text)\n",
    "            label_tokens = [zh_tokenize(r) for r in batch_data_list['all_answers'][i]]  # list of list\n",
    "            preds.append(pred_tokens)   # preds：存放模型预测的 token 列表（每条样本一个 list）\n",
    "            labels.append(label_tokens) # labels：存放该预测对应的 所有参考答案 token 列表（list of list）\n",
    "\n",
    "    # 随机打印debug_sample_num个样本\n",
    "    if debug_sample_num > 0:\n",
    "        sample_indices = random.sample(range(len(preds)), min(debug_sample_num, len(preds)))\n",
    "        print(\"\\nRandom debug samples:\")\n",
    "        for idx in sample_indices:\n",
    "            print(f\"PRED: {preds[idx]}\\nREF:  {labels[idx]}\\n\")\n",
    "\n",
    "    # 计算 BLEU-1 ~ BLEU-4\n",
    "    P1 = corpus_bleu(labels, preds, weights=(1,0,0,0))\n",
    "    P2 = corpus_bleu(labels, preds, weights=(0.5,0.5,0,0))\n",
    "    P3 = corpus_bleu(labels, preds, weights=(0.33,0.33,0.33,0))\n",
    "    P4 = corpus_bleu(labels, preds, weights=(0.25,0.25,0.25,0.25))\n",
    "\n",
    "    class Result:\n",
    "        pass\n",
    "\n",
    "    result = Result()\n",
    "    result.score = P4 * 100\n",
    "    result.precisions = (P1 * 100, P2 * 100, P3 * 100, P4 * 100)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa359a35",
   "metadata": {},
   "source": [
    "## 2.4. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "512d5cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcocora14\u001b[0m (\u001b[33mcocora14-the-university-of-sydney\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/med/wandb/run-20250928_012603-0wzum1ll</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation/runs/0wzum1ll' target=\"_blank\">t5_train</a></strong> to <a href='https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation' target=\"_blank\">https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation/runs/0wzum1ll' target=\"_blank\">https://wandb.ai/cocora14-the-university-of-sydney/text2text_generation/runs/0wzum1ll</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"text2text_generation\",\n",
    "    name=\"t5_train\",\n",
    ")\n",
    "wandb.config.update({\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": epoch_num,\n",
    "    \"batch_size\": train_batch_size,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a5469",
   "metadata": {},
   "source": [
    "### 优化器\n",
    "使用 AdamW 优化器，适合 Transformer 类模型。\n",
    "- model.parameters()：告诉优化器哪些参数需要更新。\n",
    "- lr=learning_rate：学习率。\n",
    "- 作用：控制参数更新策略（带权重衰减的 Adam）。\n",
    "\n",
    "### Accelerator 封装\n",
    "```bash\n",
    "(t5) med@server:~$ nvidia-smi\n",
    "Sun Sep 28 01:46:54 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:18:00.0 Off |                  N/A |\n",
    "| 49%   84C    P2             239W / 350W |  15720MiB / 24576MiB |     96%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA GeForce RTX 3090        Off | 00000000:3B:00.0 Off |                  N/A |\n",
    "| 30%   33C    P8              11W / 350W |      3MiB / 24576MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   2  NVIDIA GeForce RTX 3090        Off | 00000000:5E:00.0 Off |                  N/A |\n",
    "|  0%   27C    P8               9W / 370W |      3MiB / 24576MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   3  NVIDIA GeForce RTX 3090        Off | 00000000:86:00.0 Off |                  N/A |\n",
    "| 30%   28C    P8              10W / 350W |      3MiB / 24576MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A    179783      C   ...ed/anaconda3/envs/t5/bin/python3.11    15714MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "### 学习率调度器\n",
    "使用 线性衰减学习率：从初始 lr 线性下降到 0。\n",
    "\n",
    "- 参数：\n",
    "  - num_warmup_steps=0 → 没有预热阶段\n",
    "  - num_training_steps=epoch_num * len(train_dataloader) → 总训练步数\n",
    "- 作用：帮助训练更稳定，尤其在 Transformer 模型中常用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/anaconda3/envs/t5/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.523445: 100%|███████████████████████████████████████████| 1815/1815 [08:10<00:00,  3.70it/s]\n",
      "Evaluating valid:   0%|                                  | 0/88 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.590 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Evaluating valid:   1%|▎                         | 1/88 [00:01<01:43,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['B', '.', 'C', '.'] REF: [['B', '.', 'C', '.', 'E', '.'], ['B', '.', 'C', '.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   2%|▌                         | 2/88 [00:01<01:01,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['密歇根州'] REF: [['密歇根州'], ['密歇根州', '（', 'Michigan', '）'], ['Michigan']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   3%|▉                         | 3/88 [00:01<00:44,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['4.35%'] REF: [['年', '基准利率', '4.35%'], ['4.35%']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:35<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU scores: 53.62\n",
      "BLEU-1: 78.40, BLEU-2: 72.08, BLEU-3: 63.51, BLEU-4: 53.62\n",
      "saving new weights...\n",
      "\n",
      "Model weights saved to: /home/med/selflearning/stage1-t5/model/bin/epoch_1_valid_BLEU_53.62_model_weights.bin\n",
      "\n",
      "Epoch 2/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.392152: 100%|███████████████████████████████████████████| 1815/1815 [08:42<00:00,  3.48it/s]\n",
      "Evaluating valid:   1%|▎                         | 1/88 [00:00<00:27,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['10.5'] REF: [['9']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   2%|▌                         | 2/88 [00:00<00:30,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['广州', '东方', '英文', '书院'] REF: [['广州', '东方', '英文', '书院']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   3%|▉                         | 3/88 [00:00<00:27,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['4.35%'] REF: [['年', '基准利率', '4.35%'], ['4.35%']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:31<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU scores: 52.66\n",
      "BLEU-1: 78.37, BLEU-2: 71.59, BLEU-3: 62.69, BLEU-4: 52.66\n",
      "Epoch 3/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.313256: 100%|███████████████████████████████████████████| 1815/1815 [08:37<00:00,  3.51it/s]\n",
      "Evaluating valid:   1%|▎                         | 1/88 [00:00<00:32,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['550', '--', '600MM'] REF: [['以', '40', '至', '60', '厘米', '为宜'], ['40', '至', '60', '厘米']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   2%|▌                         | 2/88 [00:00<00:31,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['U', '系列'] REF: [['U', '系列']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   3%|▉                         | 3/88 [00:01<00:27,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['密歇根州'] REF: [['密歇根州'], ['密歇根州', '（', 'Michigan', '）'], ['Michigan']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:32<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU scores: 52.74\n",
      "BLEU-1: 78.69, BLEU-2: 71.89, BLEU-3: 62.88, BLEU-4: 52.74\n",
      "Epoch 4/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.259682: 100%|███████████████████████████████████████████| 1815/1815 [08:41<00:00,  3.48it/s]\n",
      "Evaluating valid:   1%|▎                         | 1/88 [00:00<00:29,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['10.5'] REF: [['9']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   2%|▌                         | 2/88 [00:00<00:30,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['广州', '东方', '英文', '书院'] REF: [['广州', '东方', '英文', '书院']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   3%|▉                         | 3/88 [00:01<00:28,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['5000', '以上'] REF: [['5000', '以上']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:35<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU scores: 55.64\n",
      "BLEU-1: 80.75, BLEU-2: 74.13, BLEU-3: 65.58, BLEU-4: 55.64\n",
      "saving new weights...\n",
      "\n",
      "Model weights saved to: /home/med/selflearning/stage1-t5/model/bin/epoch_4_valid_BLEU_55.64_model_weights.bin\n",
      "\n",
      "Epoch 5/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.221540: 100%|███████████████████████████████████████████| 1815/1815 [08:39<00:00,  3.50it/s]\n",
      "Evaluating valid:   1%|▎                         | 1/88 [00:00<00:31,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['U', '系列'] REF: [['U', '系列']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   2%|▌                         | 2/88 [00:00<00:30,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['广州', '东方', '英文', '书院'] REF: [['广州', '东方', '英文', '书院']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   3%|▉                         | 3/88 [00:01<00:28,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['4.35%'] REF: [['年', '基准利率', '4.35%'], ['4.35%']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:31<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU scores: 53.37\n",
      "BLEU-1: 79.09, BLEU-2: 72.48, BLEU-3: 63.67, BLEU-4: 53.37\n",
      "Epoch 6/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.211150:  33%|██████████████▍                             | 596/1815 [02:51<04:25,  4.59it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "loss: 0.170801: 100%|███████████████████████████████████████████| 1815/1815 [08:43<00:00,  3.47it/s]\n",
      "Evaluating valid:   1%|▎                         | 1/88 [00:00<00:27,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['550', '--', '600MM'] REF: [['以', '40', '至', '60', '厘米', '为宜'], ['40', '至', '60', '厘米']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   2%|▌                         | 2/88 [00:00<00:30,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['5000', '以上'] REF: [['5000', '以上']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   3%|▉                         | 3/88 [00:01<00:28,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['550', '--', '600MM'] REF: [['以', '40', '至', '60', '厘米', '为宜'], ['40', '至', '60', '厘米']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:31<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU scores: 52.36\n",
      "BLEU-1: 79.18, BLEU-2: 72.28, BLEU-3: 62.86, BLEU-4: 52.36\n",
      "Epoch 8/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.152993: 100%|███████████████████████████████████████████| 1815/1815 [08:44<00:00,  3.46it/s]\n",
      "Evaluating valid:   1%|▎                         | 1/88 [00:00<00:28,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['U', '系列'] REF: [['U', '系列']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   2%|▌                         | 2/88 [00:00<00:29,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['550', '--', '600MM'] REF: [['以', '40', '至', '60', '厘米', '为宜'], ['40', '至', '60', '厘米']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   3%|▉                         | 3/88 [00:00<00:26,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['10.5'] REF: [['9']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:30<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU scores: 53.05\n",
      "BLEU-1: 80.36, BLEU-2: 73.36, BLEU-3: 63.86, BLEU-4: 53.05\n",
      "Epoch 9/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.138688: 100%|███████████████████████████████████████████| 1815/1815 [08:42<00:00,  3.47it/s]\n",
      "Evaluating valid:   1%|▎                         | 1/88 [00:00<00:29,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['550', '--', '600MM'] REF: [['以', '40', '至', '60', '厘米', '为宜'], ['40', '至', '60', '厘米']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   2%|▌                         | 2/88 [00:00<00:30,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['550', '--', '600MM'] REF: [['以', '40', '至', '60', '厘米', '为宜'], ['40', '至', '60', '厘米']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid:   3%|▉                         | 3/88 [00:00<00:27,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: ['4.35%'] REF: [['年', '基准利率', '4.35%'], ['4.35%']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating valid: 100%|█████████████████████████| 88/88 [00:31<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU scores: 53.68\n",
      "BLEU-1: 80.60, BLEU-2: 73.80, BLEU-3: 64.31, BLEU-4: 53.68\n",
      "Epoch 10/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.133046:  45%|███████████████████▋                        | 810/1815 [03:50<04:05,  4.10it/s]"
     ]
    }
   ],
   "source": [
    "# 使用 AdamW 优化器\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Accelerator 封装\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, valid_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, valid_dataloader\n",
    ")\n",
    "\n",
    "# 学习率调度器\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "# 初始化指标\n",
    "total_loss = 0.\n",
    "best_bleu = 0.\n",
    "\n",
    "# 训练 + 验证循环\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_bleu = test_loop(valid_dataloader, model, mode = 'valid', debug_sample_num=10)\n",
    "\n",
    "    print(f\"Validation BLEU scores: {valid_bleu.score:.2f}\")\n",
    "    P1, P2, P3, P4 = valid_bleu.precisions\n",
    "    print(f\"BLEU-1: {P1:.2f}, BLEU-2: {P2:.2f}, BLEU-3: {P3:.2f}, BLEU-4: {P4:.2f}\")\n",
    "\n",
    "    if valid_bleu.score >= best_bleu:\n",
    "        best_bleu = valid_bleu.score\n",
    "        print('saving new weights...\\n')\n",
    "        model_path = os.path.join(save_dir, f\"epoch_{t+1}_valid_BLEU_{valid_bleu.score:0.2f}_model_weights.bin\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Model weights saved to: {model_path}\\n\")\n",
    "    \n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a18160",
   "metadata": {},
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b64b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_179490/113010269.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/med/selflearning/stage1-t5/model/bin/epoch_10_valid_BLEU_100.0000_model_weights.bin'))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/med/selflearning/stage1-t5/model/bin/epoch_10_valid_BLEU_100.0000_model_weights.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/med/selflearning/stage1-t5/model/bin/epoch_10_valid_BLEU_100.0000_model_weights.bin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      3\u001b[39m model.eval()\n\u001b[32m      5\u001b[39m sources, preds, labels = [], [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/t5/lib/python3.11/site-packages/torch/serialization.py:1065\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1063\u001b[39m     pickle_load_args[\u001b[33m'\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1067\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1068\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1069\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1070\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/t5/lib/python3.11/site-packages/torch/serialization.py:468\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    470\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/t5/lib/python3.11/site-packages/torch/serialization.py:449\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/med/selflearning/stage1-t5/model/bin/epoch_10_valid_BLEU_100.0000_model_weights.bin'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mt5_train\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250928_012245-qhxw4nri/logs\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mt5_train\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250928_012255-rjqifjt5/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/home/med/selflearning/stage1-t5/model/bin/epoch_10_valid_BLEU_100.0000_model_weights.bin'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "sources, preds, labels = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('evaluating on test set...')\n",
    "    for batch_data in tqdm(valid_dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        generated_tokens = model.generate(\n",
    "            batch_data[\"input_ids\"],\n",
    "            attention_mask=batch_data[\"attention_mask\"],\n",
    "            max_length=max_target_length,\n",
    "            num_beams=beam_size,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        ).cpu().numpy()\n",
    "\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "\n",
    "        decoded_sources = tokenizer.batch_decode(\n",
    "            batch_data[\"input_ids\"].cpu().numpy(), \n",
    "            skip_special_tokens=True, \n",
    "            use_source_tokenizer=True\n",
    "        )\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        sources += [s.strip() for s in decoded_sources]\n",
    "        preds += [p.strip() if p.strip() else 'empty' for p in decoded_preds]\n",
    "        labels += [l.strip() if l.strip() else 'empty' for l in decoded_labels]\n",
    "\n",
    "refs = [[l] for l in labels]\n",
    "bleu_score = corpus_bleu(preds, refs).score\n",
    "print(f\"Test BLEU: {bleu_score:>0.2f}\\n\")\n",
    "\n",
    "results = []\n",
    "print('saving predicted results...')\n",
    "for source, pred, label in zip(sources, preds, labels):\n",
    "    results.append({\n",
    "        \"sentence\": source, \n",
    "        \"prediction\": pred, \n",
    "        \"translation\": label \n",
    "    })\n",
    "\n",
    "with open(f'{output_dir}/test_data_pred.json', 'wt', encoding='utf-8') as f:\n",
    "    for example_result in results:\n",
    "        f.write(json.dumps(example_result, ensure_ascii=False) + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
